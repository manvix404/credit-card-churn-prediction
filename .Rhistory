df=read.csv("credit_card_churn.csv")
#Removing last 2 columns of naive bayes
df=df[,-c(22,23)]
summary(df)
head(df)
#No missing values
sum(is.na(df))
#Convert Existing Customers to 1 and Attritued to 0
df$Attrition_Flag=ifelse(df$Attrition_Flag=="Existing Customer",1,0)
library(ggplot2)
ggplot(df, aes(x = Attrition_Flag)) +
geom_bar() +
labs(title = "Count of attrition flag in train data") +
theme(plot.title = element_text(size = 20, face = "bold"))
cont_cols <- names(df)[sapply(df, is.numeric)]
#Removing CLIENTNUM field
cont_cols = cont_cols[-1]
for (i in cont_cols) {
print(ggplot(df, aes(x = i)) +
geom_bar() +
labs(title = paste("Distribution of data for", i)) +
theme_minimal() +
theme(plot.title = element_text(size = 20, face = "bold")))
}
boxplot(df['Customer_Age'])
#Removing outlier where age>68 and setting it to mean age
df$Customer_Age[df$Customer_Age > 68] <- mean(df$Customer_Age, na.rm = TRUE)
boxplot(df['Customer_Age'])
ggplot(df, aes(x = Card_Category)) +
geom_bar() +
labs(title = "Card category counts") +
theme_minimal() +
theme(plot.title = element_text(size = 20, face = "bold"))
#Platinum and Gold are the outliers
#Imputing 'Gold' & 'Platinum' Card_Category with the 'Silver' Card_Category.
df$Card_Category <- ifelse(df$Card_Category == "Gold" | df$Card_Category == "Platinum","Silver",
df$Card_Category)
ggplot(df, aes(x = Card_Category)) +
geom_bar() +
labs(title = "Card category counts") +
theme_minimal() +
theme(plot.title = element_text(size = 20, face = "bold"))
library(caTools)
set.seed(123321)
split=sample.split(df,SplitRatio = 0.8)
train=subset(df,split=="TRUE")
test=subset(df,split=="FALSE")
head(train)
model=glm(Attrition_Flag~Customer_Age+Dependent_count+Months_on_book+Total_Relationship_Count+Months_Inactive_12_mon+Contacts_Count_12_mon+Credit_Limit+Total_Revolving_Bal+Avg_Open_To_Buy+Total_Amt_Chng_Q4_Q1+Total_Trans_Amt+Total_Trans_Ct+Total_Ct_Chng_Q4_Q1+Avg_Utilization_Ratio,data=train,family="binomial")
model
#Predict
predicted=predict(model,test,type="response")
#Change probabilites
predicted=ifelse(predicted>0.5,1,0)
head(predicted)
#Model Accuracy
table(test$Attrition_Flag,predicted)
classerr=mean(predicted!=test$Attrition_Flag)
paste("Accuracy is ",1-classerr)
library(caTools)
set.seed(123321)
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
split=sample.split(df,SplitRatio = 0.8)
train=subset(df,split=="TRUE")
test=subset(df,split=="FALSE")
head(train)
model=glm(Attrition_Flag~Customer_Age+Dependent_count+Months_on_book+Total_Relationship_Count+Months_Inactive_12_mon+Contacts_Count_12_mon+Credit_Limit+Total_Revolving_Bal+Avg_Open_To_Buy+Total_Amt_Chng_Q4_Q1+Total_Trans_Amt+Total_Trans_Ct+Total_Ct_Chng_Q4_Q1+Avg_Utilization_Ratio,data=train,family="binomial")
model
#Predict
predicted=predict(model,test,type="response")
#Change probabilites
predicted=ifelse(predicted>0.5,1,0)
head(predicted)
#Model Accuracy
table(test$Attrition_Flag,predicted)
classerr=mean(predicted!=test$Attrition_Flag)
paste("Accuracy is ",1-classerr)
library(party)
#train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)], as.factor)
#test[sapply(test, is.character)] <- lapply(test[sapply(test, is.character)], as.factor)
classifier_cl <- ctree(Attrition_Flag~.,data=train)
#Fitting Decision Tree Model to training dataset
plot(classifier_cl)
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test)
# Confusion Matrix
cm <- table(test$Attrition_Flag, y_pred)
cm
# Model Evaluation
#confusionMatrix(cm)
#Accuracy
accuracy = mean(y_pred!=test$Attrition_Flag)
accuracy = 1 - accuracy
print(paste('Accuracy of the model = ',accuracy))
